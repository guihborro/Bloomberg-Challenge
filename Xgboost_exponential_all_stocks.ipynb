{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preâmbulo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.api as qqplot\n",
    "import random\n",
    "import yfinance as yf\n",
    "import os\n",
    "\n",
    "# bibiotecas Time_series\n",
    "import matplotlib.dates as mdates\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMAResults\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "## Estatísticas de ajuste:\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Outros modelos de regressão\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('C:/Users/jpzam/Desktop/CHALLENGE_PERSONAL/data/ativos_csv_2023.csv')\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.set_index('Date', inplace= True)\n",
    "df.index = pd.to_datetime(df.index, format = '%Y-%m-%d')\n",
    "\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importando Bitcoin; fazendo join em 'df'\n",
    "\n",
    "bit_df = yf.download('BTC', start = '2023-01-03')\n",
    "\n",
    "bit_df = bit_df[['Close']].rename(columns = {'Close':'BTC'})\n",
    "\n",
    "bit_df.index = pd.to_datetime(bit_df.index, format = '%Y-%m-%d')\n",
    "\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df_start = df.merge(bit_df, left_index=True, right_index=True, how = 'left').copy()\n",
    "\n",
    "df_start.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Um XGBoost para cada ativos dos 61;\n",
    "\n",
    "#for col in df.columns\n",
    "def stock_predict(col, df):\n",
    "\n",
    "    forecast_df = pd.DataFrame()\n",
    "\n",
    "    df_col = df[[col]].dropna().copy()\n",
    "\n",
    "    ## Fazendo o train-test split (10)\n",
    "\n",
    "    len_df = len(df_col)\n",
    "    train_size = int(len_df*0.8)\n",
    "\n",
    "    #print(len_df)\n",
    "\n",
    "    train = df_col.iloc[:train_size]\n",
    "    train.index = pd.to_datetime(train.index)\n",
    "    train = train.asfreq('D', method = 'ffill')\n",
    "\n",
    "    test = df_col.iloc[train_size:]\n",
    "    test.index = pd.to_datetime(test.index)\n",
    "    test = test.asfreq('D', method = 'ffill')\n",
    "\n",
    "    #print(train)\n",
    "    #print(test)\n",
    "\n",
    "    df_col = df_col.asfreq('D', method = 'ffill')\n",
    "\n",
    "    merged_df = df_col.copy() #pd.merge(df_final, avir_df, how='left', right_index=True, left_index=True)\n",
    "\n",
    "    merged_df.tail(10)\n",
    "\n",
    "    ## Calculando as Trends\n",
    "\n",
    "\n",
    "    def des_optimizer(train, alphas, betas, step=int(len(test))):\n",
    "        best_alpha, best_beta, best_mae = None, None, float(\"inf\")\n",
    "        for alpha in alphas:\n",
    "            for beta in betas:\n",
    "                des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=alpha, smoothing_trend=beta)\n",
    "                y_pred = des_model.forecast(step)\n",
    "                mae = mean_absolute_error(test, y_pred)\n",
    "                if mae < best_mae:\n",
    "                    best_alpha, best_beta, best_mae = alpha, beta, mae\n",
    "                #print(\"alpha:\", round(alpha, 2), \"beta:\", round(beta, 2), \"mae:\", round(mae, 4))\n",
    "        print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_mae:\", round(best_mae, 4))\n",
    "        return best_alpha, best_beta, best_mae\n",
    "\n",
    "    alphas = np.arange(0.01, 1, 0.10)\n",
    "    betas = np.arange(0.01, 1, 0.10)\n",
    "\n",
    "    best_alpha, best_beta, best_mae = des_optimizer(train, alphas, betas)\n",
    "\n",
    "    decomp = ExponentialSmoothing(merged_df[col].dropna(), trend='multiplicative')\n",
    "    fit = decomp.fit(smoothing_level=0.11, smoothing_trend=0.01)\n",
    "    merged_df['trend'] = fit.fittedvalues\n",
    "\n",
    "    fcast = fit.forecast(25)\n",
    "\n",
    "    merged_df[[col,'trend']].plot()\n",
    "\n",
    "    fcast.plot() \n",
    "\n",
    "    ## Acoplando FCAST\n",
    "\n",
    "    fcast.index.name = 'Date'\n",
    "\n",
    "    fcast_df = pd.DataFrame(columns = merged_df.columns, index = fcast.index)\n",
    "    fcast_df['trend'] = fcast.values\n",
    "    fcast_df[col] = np.nan\n",
    "\n",
    "    fcast_df.head()\n",
    "\n",
    "    # Juntando no Merged_df\n",
    "    merged_df = pd.concat([merged_df, fcast_df])\n",
    "\n",
    "    #### CRIANDO AS FEATURES ####\n",
    "    # ## Features 1 - Preços lags\n",
    "\n",
    "    #merged_df['lag_3'] = merged_df['AVIR'].shift(3).fillna(method = 'ffill')\n",
    "    merged_df['lag_5'] = merged_df[col].shift(5)\n",
    "    merged_df['lag_7'] = merged_df[col].shift(7)\n",
    "    merged_df['lag_10'] = merged_df[col].shift(10)\n",
    "    merged_df['lag_15'] = merged_df[col].shift(15)\n",
    "    merged_df['lag_30'] = merged_df[col].shift(30)\n",
    "\n",
    "\n",
    "    ## Criando as Moving averages\n",
    "    # Step 1: Calculate EMAs\n",
    "    merged_df['EMA_9'] = merged_df[col].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "    # Step 2: Calculate SMAs\n",
    "    merged_df['SMA_5'] = merged_df[col].rolling(window=5).mean()\n",
    "    merged_df['SMA_15'] = merged_df[col].rolling(window=15).mean()\n",
    "    merged_df['SMA_30'] = merged_df[col].rolling(window=30).mean()\n",
    "\n",
    "    # Step 3: Calculate RSI\n",
    "    def calculate_rsi(data, window=14):\n",
    "        delta = data.diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "        rs = gain / loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "\n",
    "    merged_df['RSI'] = calculate_rsi(merged_df[col], window=14)\n",
    "\n",
    "    merged_df['RSI_lag_5'] = merged_df['RSI'].shift(5)\n",
    "    merged_df['RSI_lag_7'] = merged_df['RSI'].shift(7)\n",
    "    merged_df['RSI_lag_10'] = merged_df['RSI'].shift(10)\n",
    "    merged_df['RSI_lag_15'] = merged_df['RSI'].shift(15)\n",
    "    merged_df['RSI_lag_30'] = merged_df['RSI'].shift(30)\n",
    "\n",
    "    # Step 4: Calculate MACD\n",
    "    merged_df['EMA_12'] = merged_df[col].ewm(span=12, adjust=False).mean()\n",
    "    merged_df['EMA_26'] = merged_df[col].ewm(span=26, adjust=False).mean()\n",
    "    #merged_df['MACD'] = merged_df['EMA_12'] - merged_df['EMA_26']\n",
    "    #merged_df['MACD_SIGNAL'] = merged_df['MACD'].ewm(span=9, adjust=False).mean()  # Signal line\n",
    "\n",
    "        ## VAR PERCENTUAL DOS SMAs\n",
    "    pct_5 = merged_df['SMA_5'].dropna().pct_change().iloc[-1]\n",
    "    pct_15 = merged_df['SMA_15'].dropna().pct_change().iloc[-1]\n",
    "    pct_30 = merged_df['SMA_30'].dropna().pct_change().iloc[-1]\n",
    "    #pct_12 = merged_df['EMA_12'].dropna().pct_change().iloc[-1]\n",
    "    #pct_26 = merged_df['EMA_26'].dropna().pct_change().iloc[-1]\n",
    "\n",
    "    #for na in range(len(merged_df['SMA_30'].isna())):\n",
    "    #    merged_df['EMA_12'] = np.where(merged_df['SMA_30'].isna(), np.nan, merged_df['EMA_12'])\n",
    "    #for na in range(len(merged_df['SMA_30'].isna())):\n",
    "    #    merged_df['EMA_12'] = merged_df['EMA_12'].fillna(merged_df['EMA_12'].shift() * (1 + pct_30))\n",
    "\n",
    "    for na in range(len(merged_df['SMA_30'].isna())):\n",
    "        merged_df['EMA_26'] = np.where(merged_df['SMA_30'].isna(), np.nan, merged_df['EMA_26'])\n",
    "    for na in range(len(merged_df['SMA_30'].isna())):\n",
    "        merged_df['EMA_26'] = merged_df['EMA_26'].fillna(merged_df['EMA_26'].shift() * (1 + pct_30))\n",
    "\n",
    "    for na in range(len(merged_df['SMA_5'].isna())):\n",
    "        merged_df['SMA_5'] = merged_df['SMA_5'].fillna(merged_df['SMA_5'].shift() * (1 + pct_5))\n",
    "        \n",
    "    for na in range(len(merged_df['SMA_15'].isna())):\n",
    "        merged_df['SMA_15'] = merged_df['SMA_15'].fillna(merged_df['SMA_15'].shift() * (1 + pct_15))\n",
    "\n",
    "    for na in range(len(merged_df['SMA_30'].isna())):\n",
    "        merged_df['SMA_30'] = merged_df['SMA_30'].fillna(merged_df['SMA_30'].shift() * (1 + pct_30))\n",
    "\n",
    "    merged_df['MACD'] = merged_df['EMA_12'] - merged_df['EMA_26']\n",
    "    merged_df['MACD_SIGNAL'] = merged_df['MACD'].ewm(span=9, adjust=False).mean()  # Signal line\n",
    "\n",
    "    merged_df = merged_df.drop(merged_df.index[:43])\n",
    "    merged_df.dropna(subset = 'lag_5', inplace = True)  \n",
    "\n",
    "\n",
    "    ## SEPARANDO TREINO PARA A SELEÇÃO DE FEATURES VIA LASSO\n",
    "    # ## Separação treino\n",
    "    train_size = int(len(merged_df)*0.8)\n",
    "    train_df = merged_df.iloc[:train_size]\n",
    "    \n",
    "    # Scaling das Fatures do Modelo Lasso\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df.drop(col, axis=1))\n",
    "\n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.drop(col, axis=1).columns, index=train_df.index)\n",
    "\n",
    "    # Merge the scaled features with the target variable\n",
    "    train_scaled_df[col] = train_df[col]\n",
    "\n",
    "    #test_scaled_df = test_scaled_df.dropna()\n",
    "\n",
    "    # Split the scaled data into Features and Label\n",
    "    y_train = train_scaled_df[col]\n",
    "    X_train = train_scaled_df.drop(col, axis=1)\n",
    "\n",
    "    # Parâmetros GridSearchCV\n",
    "    params = {\"alpha\":np.logspace(-4, 4, num=100)}#0.00001, 10, 500)}\n",
    "\n",
    "    # N Folds \n",
    "    kf = KFold(n_splits=5,shuffle=True, random_state=42)\n",
    "\n",
    "    # Modelo Lasso\n",
    "    lasso = Lasso(max_iter = 10000)\n",
    "\n",
    "    # GridSearch\n",
    "    lasso_cv = GridSearchCV(lasso, param_grid = params, cv = kf)\n",
    "    lasso_cv.fit(X_train, y_train)\n",
    "    print(\"Best Params {}\".format(lasso_cv.best_params_))\n",
    "\n",
    "    ## Nome das colunas\n",
    "    names = merged_df.drop(col, axis=1).columns\n",
    "    print(\"Column Names: {}\".format(names.values))\n",
    "\n",
    "    alpha_select = list(lasso_cv.best_params_.values())[0]\n",
    "\n",
    "    lasso1 = Lasso(alpha = alpha_select)\n",
    "    lasso1.fit(X_train, y_train)\n",
    "\n",
    "    # Coeficientes absoutos \n",
    "    lasso1_coef = np.abs(lasso1.coef_)\n",
    "\n",
    "    # Plot\n",
    "    plt.bar(names, lasso1_coef)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid()\n",
    "    plt.title(\"Feature Selection Based on Lasso\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    #plt.ylim(0, 100)\n",
    "    plt.show()\n",
    "\n",
    "    ## Criando features de exclusão\n",
    "    laglist = ['lag_5','lag_7', 'lag_10', 'lag_15', 'lag_30','trend']\n",
    "    techlist = ['EMA_9','EMA_12','EMA_26','SMA_5','SMA_15','SMA_30']\n",
    "    MACDlist = ['MACD','MACD_SIGNAL']\n",
    "    rsilist = ['RSI', 'RSI_lag_7','RSI_lag_5','RSI_lag_10','RSI_lag_15','RSI_lag_30']\n",
    "\n",
    "    lista_de_listas_exclusao = [laglist, techlist, MACDlist, rsilist] #EMAlist, SMAlist, MACDlist]\n",
    "\n",
    "    # Excluindo features 0.01\n",
    "    feature_subset = np.array(names)[lasso1_coef>0.01]\n",
    "    print(\"Selected Features: {}\".format(feature_subset))\n",
    "\n",
    "    # Appendando AVIR \n",
    "    feature_select = np.append(feature_subset, col)\n",
    "    print(\"Selected Cols: {}\".format(feature_select))\n",
    "\n",
    "    ## Loop de seleção das exclusões:\n",
    "    droplist = np.array([])\n",
    "    # Iniciando loop\n",
    "    for exlist in lista_de_listas_exclusao:\n",
    "        mask = np.isin(names, exlist)\n",
    "        #print(mask)\n",
    "\n",
    "        ex_select = np.array(names)[mask]\n",
    "        coef_select = np.array(lasso1_coef)[mask]\n",
    "        #print(ex_select)\n",
    "        #print(coef_select)\n",
    "\n",
    "        if coef_select.size > 0:\n",
    "            max_index = np.argmax(coef_select)  # Índice do valor máximo\n",
    "            max_value = coef_select[max_index]   # Valor máximo\n",
    "            max_name = ex_select[max_index] \n",
    "        else:\n",
    "            #pass\n",
    "            max_name = []\n",
    "\n",
    "        #print(max_name)\n",
    "\n",
    "        ## Selecionado aqui o nomes que vamos droppa de names\n",
    "        names_to_drop = np.delete(exlist, np.isin(exlist, max_name))\n",
    "        #print(names_to_drop)\n",
    "\n",
    "        droplist = np.append(droplist, names_to_drop)\n",
    "        print(droplist)\n",
    "\n",
    "    feature_select_excluded = np.delete(feature_select, np.isin(feature_select, droplist))\n",
    "\n",
    "    merged_df = merged_df[feature_select_excluded]\n",
    "\n",
    "    ## REPETINDO O SCALING E FAZENDO DE FATO O SPLIT TRAIN-TEST\n",
    "    ## Separação treino test\n",
    "    train_size = int(len(merged_df)*0.8)\n",
    "    train_df, test_df = merged_df.iloc[:train_size], merged_df.iloc[train_size:]\n",
    "\n",
    "    #test_df\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train_df.drop(col, axis=1))\n",
    "    test_scaled = scaler.transform(test_df.drop(col, axis=1))\n",
    "\n",
    "    # Convert the scaled data back to a DataFrame\n",
    "    train_scaled_df = pd.DataFrame(train_scaled, columns=train_df.drop(col, axis=1).columns, index=train_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_scaled, columns=test_df.drop(col, axis=1).columns, index=test_df.index)\n",
    "\n",
    "    # Merge the scaled features with the target variable\n",
    "    train_scaled_df[col] = train_df[col]\n",
    "    test_scaled_df[col] = test_df[col]\n",
    "\n",
    "    forecast_df = test_scaled_df[test_scaled_df[col].isna()]\n",
    "\n",
    "    test_scaled_df = test_scaled_df.dropna()\n",
    "\n",
    "    # Split the scaled data into Features and Label\n",
    "    y_train = train_scaled_df[col]\n",
    "    X_train = train_scaled_df.drop(col, axis=1)\n",
    "    y_test = test_scaled_df[col]\n",
    "    X_test = test_scaled_df.drop(col, axis=1)\n",
    "\n",
    "    #forecast_df ; X_forecast, df de forecat do XGBoost\n",
    "    X_forecast = forecast_df.drop(col, axis=1)\n",
    "    X_forecast\n",
    "\n",
    "    ## Iniciando o modelo XGBoost ##\n",
    "\n",
    "    # Criando o score MAPE:\n",
    "    def mape(y_true, y_pred):\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "    mape_scorer = make_scorer(mape, greater_is_better=False)  # MAPE should be minimized\n",
    "\n",
    "    ## Estimando os melhores hyperparametros para o modelo XGBoost\n",
    "    parameters = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05],\n",
    "    'max_depth': [8, 10, 12, 15],\n",
    "    'gamma': [0.001, 0.005, 0.01, 0.02],\n",
    "    #'alpha': [0.1, 0.5, 1],  # Regularização L1\n",
    "    'lambda': [0.5, 1, 1.5] # Regularização L2\n",
    "    }\n",
    "\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    model = xgb.XGBRegressor(objective='reg:squarederror', verbosity=1, random_state = 42, early_stopping_rounds = 20)\n",
    "    #clf = GridSearchCV(model, parameters, scoring={'MAPE': mape_scorer}, refit = 'MAPE', cv=5)\n",
    "\n",
    "    clf = RandomizedSearchCV(model, parameters, scoring='neg_mean_absolute_error', refit=True, cv=10, n_iter=50, random_state = 42)  # Random Search para um modelo menos lento (?) O pc sofre.\n",
    "\n",
    "    clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    print(f'Best params: {clf.best_params_}')\n",
    "    print(f'Best validation score = {clf.best_score_}')\n",
    "\n",
    "    ## MODELO ESEMBLE XGB + LASSO + RIDGE + GBOOST + MLP\n",
    "    base_estimators = [\n",
    "    ('xgb', xgb.XGBRegressor(objective='reg:squarederror', **clf.best_params_, random_state = 42)),  # Best params from RandomizedSearchCV\n",
    "    ('ridge', Ridge()),\n",
    "    ('lasso', Lasso()),\n",
    "    #('catboost', CatBoostRegressor(verbose=0, random_state=42)),\n",
    "    #('adaboost', AdaBoostRegressor(random_state=42)),\n",
    "    ('gboost', GradientBoostingRegressor(random_state=42)),\n",
    "    #('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('mlp', MLPRegressor(hidden_layer_sizes = (8,16,8), max_iter = 2000, random_state=42)) \n",
    "    ]\n",
    "\n",
    "    reg = StackingRegressor(\n",
    "        estimators=base_estimators,\n",
    "        final_estimator= xgb.XGBRegressor(),  \n",
    "        cv = 10\n",
    "    )\n",
    "\n",
    "    reg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    ## erros e avaliação do modelo\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    evs = explained_variance_score(y_test, y_pred)\n",
    "\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    mpe = np.mean((y_test - y_pred) / y_test) * 100\n",
    "\n",
    "\n",
    "    print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    print(f\"Explained Variance Score: {evs}\")\n",
    "    print(f\"Mean Absolute Percentage Error (MAPE): {mape}\")\n",
    "    print(f\"Mean Percentage Error (MPE): {mpe}\")\n",
    "    \n",
    "    y_forecast = reg.predict(X_forecast)\n",
    "\n",
    "    # Plot final Predictions\n",
    "    plt.figure(figsize=(12, 6), dpi=100)\n",
    "    plt.plot(test_scaled_df.index, y_test, label='Real')\n",
    "    plt.plot(test_scaled_df.index, y_pred, color='red', label='Predicted')\n",
    "    plt.plot(forecast_df.index, y_forecast, color = 'blue', label = 'Forecast')\n",
    "\n",
    "    mean_y_test = y_test.mean()\n",
    "    # Linha horizontal\n",
    "    plt.axhline(mean_y_test, color='grey', linestyle='--', label='Historical Mean')\n",
    "\n",
    "    plt.title('Model Predictions vs Real Prices')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Stock Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    ## Retorna nada se o mape estiver ruim demais. Ainda assim printa, para conferêcia\n",
    "    if mape > 5:\n",
    "        print(f'MAPE for {col} is {mape:.2f}, skipping this column.')\n",
    "        return None  # Retorne None se o MAPE for maior que 6\n",
    "    \n",
    "    ## appendando os resultado em um result_df\n",
    "    result_df = pd.DataFrame(y_forecast, columns=[col], index=forecast_df.index)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Crando o df de forecasts para acumular em loop, vazio.\n",
    "\n",
    "all_forecasts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# ProcessPoolExecutor\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    futures = {executor.submit(stock_predict, col, df): col for col in df.columns}\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        col = futures[future]\n",
    "        try:\n",
    "            result_df = future.result()\n",
    "            all_forecasts = pd.concat([all_forecasts, result_df], axis=1)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing {col}: {e}')\n",
    "\n",
    "all_forecasts.columns = df.columns\n",
    "\n",
    "print(all_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fazendo o concat. \n",
    "\n",
    "df_final = all_forecasts.copy()\n",
    "\n",
    "df_final.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pct = df_final.pct_change(periods = 5)*100\n",
    "\n",
    "last_var = df_pct.iloc[-1]\n",
    "\n",
    "top_5 = last_var.nlargest(10).index.tolist()\n",
    "\n",
    "print(top_5)\n",
    "\n",
    "df_select = df_final[top_5]\n",
    "\n",
    "df_select.tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
